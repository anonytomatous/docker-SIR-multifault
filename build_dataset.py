"""
build_dataset.py

generates json dataset containing single/multi-faults data for failure clustering experiment.


Internally, this script implements several post-processing steps on the raw data generated by test suite execution for each faulty versions.

1. Clean failing test results (filtering out test cases that crashed with segmentation fault that does not generate coverage) 

2. Convert the raw coverage files into coverage matrix dataframes in `coverage_files/dataframes` directory

3. Establish mapping of each fault and failing tests for clustering ground-truth

4. Save the location of faulty lines to files
"""

import os
import re
import glob
import pandas as pd
import argparse
import shutil
import json
import logging

from faultmap import get_fault_map

logging.basicConfig(filename='dataset_generation.log',
                    filemode='w', level=logging.INFO)


def convert2dataframe(cov_dir):
    # row: statement, column: test case
    cov_files = glob.glob(os.path.join(cov_dir, '*.gcov'))
    if len(cov_files) == 0:
        print('coverage files not yet generated')
        return None, False

    columns = []
    for cov_file in cov_files:
        tc_name = os.path.basename(cov_file).split('.')[0]
        tc_cov = get_tc_cov(cov_file)
        # FIXME: assert faulty element is covered by failing test
        tc_cov.columns = [tc_name]
        columns.append(tc_cov)

    return pd.concat(columns, axis=1), True


def get_tc_cov(tc_gcov_path):
    with open(tc_gcov_path) as f:
        lines = f.readlines()

    column = dict()
    for i, l in enumerate(lines):
        m = re.search(r'([0-9\-\#\s]+):([0-9\s]+):(.*)', l)

        hit_count = m.group(1).strip()
        line_no = int(m.group(2).strip())
        code_line = m.group(3).strip()

        if hit_count == '-':
            continue
        elif '#' in hit_count:
            hit_count = 0
        else:
            hit_count = int(hit_count)

        key = ('_', str(line_no))
        column[key] = hit_count

    return pd.DataFrame(column.values(), index=column.keys())


def load_coverage_pickle(project, version, faultid, faulty_line_no):
    if not os.path.exists(f'./coverage_files/dataframes/{project}/{version}/{faultid}.pkl'):
        logging.info(
            f'No coverage dataframe for version {project}-{version}-{faultid}: faulty line {faulty_line_no}')
        logging.info(
            f'./coverage_files/dataframes/{project}/{version}/{faultid}.pkl')
        return None

    print(
        f'loading coverage from: ./coverage_files/dataframes/{project}/{version}/{faultid}.pkl')

    df = pd.read_pickle(
        f'./coverage_files/dataframes/{project}/{version}/{faultid}.pkl')
    df.index = pd.MultiIndex.from_tuples(df.index, names=['stmt', 'lineno'])

    try:
        faulty_line = df.xs(str(faulty_line_no),
                            level='lineno', drop_level=False)
    except KeyError as e:
        # The faulty line is in non-executable-region (e.g. global variable declaration)
        logging.info(
            f'faulty line is not added - not in coverage matrix (line number: {e})')
        logging.info(f'{project}-{version}-{faultid}:{faulty_line_no}')
        return None

    assert len(faulty_line) == 1

    return faulty_line.index[0]


def save_faulty_components(project):
    with open(f'./faulty_lines/{project}_faulty_lines.json') as f:
        faulty_lines_dict = json.load(f)

    for version in faulty_lines_dict:
        result_path = f'./faulty_lines/{project}/{version}'

        if os.path.exists(result_path):
            shutil.rmtree(result_path)
        os.makedirs(result_path)

        for faultid in faulty_lines_dict[version]:
            faulty_lines = faulty_lines_dict[version][faultid]

            faulty_components = []
            for faulty_line_no in faulty_lines:
                faulty_line_index = load_coverage_pickle(
                    project, version, faultid, faulty_line_no)
                if faulty_line_index == None:
                    continue
                faulty_components.append(faulty_line_index)

            with open(f'{result_path}/{faultid}', 'w') as f:
                f.writelines([f'{comp[0]},{comp[1]}' +
                              '\n' for comp in faulty_components])


def check_subsume(individual_failing_tests):
    overlapped_tests = []
    individual_failing_tests = list(individual_failing_tests)

    for i in range(len(individual_failing_tests)):
        for j in range(i+1, len(individual_failing_tests)):
            overlapped_tests.extend(list(set.intersection(
                individual_failing_tests[i], individual_failing_tests[j])))

    overlapped_tests = set(overlapped_tests)

    subsumed = False

    for tests in individual_failing_tests:
        if len(tests - overlapped_tests) == 0:
            subsumed = True
            break

    logging.debug(overlapped_tests)
    return subsumed


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', '-p', default='gzip')
    parser.add_argument('--version', '-v', nargs='+')
    parser.add_argument('--outfile', '-o', default='./result.json')
    args = parser.parse_args()

    dataset = dict()
    faultmap = get_fault_map(args.project)
    faultmap = faultmap.set_index('version_id')
    print(
        f'All versions of which failing tests are observed: {len(faultmap.index)}')
    print(f'List of candidate versions: {list(faultmap.index)}')

    file_dirname = os.path.dirname(os.path.abspath(__file__))
    subsumed_datapoints = []

    def add_to_dataset(dataset, project, version, individual_faults, failing_tests_valid, combined_faultid=None):

        # 2. Convert the raw coverage files into coverage matrix dataframes in `coverage_files/dataframes` directory
        cov_dir = os.path.join(
            f'./coverage_files/{project}/{version}', faultid)
        cov_pickle_path = f'./coverage_files/dataframes/{project}/{version}/' + \
            f'{faultid}.pkl'

        if os.path.exists(cov_pickle_path):
            pass
        else:
            df, success = convert2dataframe(cov_dir)

            if success == False:
                raise Exception('convert2dataframe failed')

            os.makedirs(
                os.path.dirname(cov_pickle_path), exist_ok=True)

            df.to_pickle(cov_pickle_path, protocol=4)

        info = {
            'coverage': os.path.join(file_dirname, cov_pickle_path),
            'setup_command': "echo 'no setup command yet'",
            'cleanup_command': "",
            'failing_tests': {
            },
            'faulty_components': {
            },
            'faulty_components_level': 1
        }

        individual_failing_tests = []

        # 3. Establish mapping of each fault and failing tests for clustering ground-truth
        for id in individual_faults:
            if combined_faultid == None:
                individual_faultid = f'{version}_{id}'
            else:
                individual_faultid = f'{version}_{combined_faultid}_{id}'

            i_vid = f'{version}_{id}'

            # Clustering GT: failing_tests_(A_B_C)_(A) = (test_A_B_C’) ∩ (test_A’)
            i_failing_tests = set(valid_faultmap.loc[i_vid]['failing_tests_valid']).intersection(
                set(failing_tests_valid))
            individual_failing_tests.append(i_failing_tests)

            os.makedirs(f'./failing_tests_valid/{args.project}', exist_ok=True)
            with open(f'./failing_tests_valid/{args.project}/{individual_faultid}', 'w') as f:
                f.writelines([l + '\n' for l in sorted(i_failing_tests)])

            info['failing_tests'][f'{project}-{version}-{id}'] = os.path.join(
                file_dirname, f'failing_tests_valid/{project}/{individual_faultid}')
            info['faulty_components'][f'{project}-{version}-{id}'] = os.path.join(
                file_dirname, f'faulty_lines/{project}/{version}/{id}')

        if check_subsume(individual_failing_tests):
            subsumed_datapoints.append(f'{project}-{version}-{faultid}')

        dataset[f'{project}-{version}-{faultid}'] = info
        return True

    if args.version == None:
        versionpath = f'./coverage_files/{args.project}'
        versions = [dirname for dirname in os.listdir(
            versionpath) if os.path.isdir(os.path.join(versionpath, dirname))]
    else:
        versions = args.version

    new_faultmap_rows = []

    # 1. Clean failing test results (filtering out test cases that crashed with segmentation fault that does not generate coverage)
    for vid in faultmap[(faultmap['faults_num'] == 1) & (faultmap['nonempty'] == True)].index:
        failing_tests = faultmap.loc[vid].failing_tests
        version = faultmap.loc[vid].original_version
        faultid = faultmap.loc[vid].faultid

        non_segfault_failing_tests = []
        segfault_tests = []

        for testname in failing_tests:
            covfiles = os.listdir(
                f'./coverage_files/{args.project}/{version}/{faultid}')
            if f'{testname}.gcov' in covfiles:
                non_segfault_failing_tests.append(testname)
            else:
                segfault_tests.append(testname)

        row = dict(faultmap.loc[vid])
        row['version_id'] = vid
        row['failing_tests_valid'] = list(set(non_segfault_failing_tests))
        row['segfault_tests'] = segfault_tests
        row['has_segfault'] = len(segfault_tests) > 0
        row['valid'] = len(non_segfault_failing_tests) > 1

        new_faultmap_rows.append(row)

    sfm = pd.DataFrame(new_faultmap_rows)
    # sfm.to_pickle(f'singlefaultmap_{args.project}.pkl')
    sfm = sfm.set_index('version_id')

    multifault_vids = faultmap[(faultmap['faults_num'] > 1)].index
    for i, vid in enumerate(multifault_vids):
        failing_tests = faultmap.loc[vid].failing_tests
        activated_faults = faultmap.loc[vid].activated_faults
        version = faultmap.loc[vid].original_version
        faultid = faultmap.loc[vid].faultid

        print(
            f'processing {version}_{faultid} ({i+1}/{len(multifault_vids)})')

        non_segfault_failing_tests = []
        segfault_tests = []

        if not os.path.exists(f'./coverage_files/{args.project}/{version}/{faultid}'):
            continue
        
        covfiles = os.listdir(
            f'./coverage_files/{args.project}/{version}/{faultid}')

        for testname in failing_tests:
            if f'{testname}.gcov' in covfiles:
                non_segfault_failing_tests.append(testname)
            else:
                segfault_tests.append(testname)

        # 'Clusterable' data: all faults should be discoverable & should have no extra failure
        # (test_A_B_C’) is subset of (test_A’) U (test_B’) U (test_C’)
        # (test_A_B_C’) intersection (test_A’) != empty
        # (test_A_B_C’) intersection (test_B’) != empty
        # (test_A_B_C’) intersection (test_C’) != empty

        individual_failing_tests = []
        for i_faultid in activated_faults:
            i_vid = f'{version}_{i_faultid}'
            try:
                individual_failing_tests.append(
                    set(sfm.loc[i_vid].failing_tests_valid))
            except KeyError:
                assert len(faultmap.loc[i_vid].failing_tests) == 0

        no_extra_failure = set(non_segfault_failing_tests).issubset(
            set.union(*individual_failing_tests))
        no_masked_failure = True
        for i_failing_tests in individual_failing_tests:
            if len(set(non_segfault_failing_tests).intersection(i_failing_tests)) == 0:
                no_masked_failure = False

        valid = no_extra_failure and no_masked_failure

        row = dict(faultmap.loc[vid])
        row['version_id'] = vid
        row['failing_tests_valid'] = non_segfault_failing_tests
        row['segfault_tests'] = segfault_tests
        row['has_segfault'] = len(segfault_tests) > 0
        row['valid'] = valid

        new_faultmap_rows.append(row)

    valid_faultmap = pd.DataFrame(new_faultmap_rows)
    # valid_faultmap.to_pickle(f'faultmap_{args.project}.pkl')
    valid_faultmap = valid_faultmap.set_index('version_id')

    all_valid_versions = valid_faultmap[(
        valid_faultmap['valid'] == True)].index

    for i, vid in enumerate(all_valid_versions):
        version = valid_faultmap.loc[vid].original_version
        faultid = valid_faultmap.loc[vid].faultid
        faults_num = valid_faultmap.loc[vid].faults_num
        non_segfault_failing_tests = valid_faultmap.loc[vid].failing_tests_valid
        print(
            f'processing {version}_{faultid} ({i+1}/{len(all_valid_versions)})')

        if len(non_segfault_failing_tests) > 1:
            if faults_num == 1:
                add_to_dataset(dataset, args.project, version,
                               valid_faultmap.loc[vid].activated_faults, non_segfault_failing_tests, combined_faultid=None)
            else:
                add_to_dataset(dataset, args.project, version,
                               valid_faultmap.loc[vid].activated_faults, non_segfault_failing_tests, combined_faultid=faultid)

    # 4. Save the location of faulty lines to files
    save_faulty_components(args.project)

    print(f'# of datapoints: {len(dataset)}')
    print(f'# of subsumed: {len(subsumed_datapoints)}')
    print(
        f'# of remaining datapoints: {len(dataset) - len(subsumed_datapoints)}')
    with open(f'{args.outfile}', 'w') as f:
        json.dump(dataset, f, indent=4)
